# NLP_toxic_comment_detector
Natural Language Processor. Takes comments and identifies them as either hate speech, offensive language or neither. The model is a Neural Network with an embedding layer, a bidirectional LSTM layer, 3 hidden dense layers and the output layer. The model is trained on 'Hate Speech and Offensive Language Dataset' on Kaggle: https://www.kaggle.com/datasets/mrmorj/hate-speech-and-offensive-language-dataset/data. This project is inspired by a youtube tutorial (Build a Comment Toxicity Model with Deep Learning and Python by Nicholas Renette).
## Model
The model has an accuracy of around 0.99 and performs extremely well on my test data, however experiences some misclassifications in a few examples i tried on my own. I chose an embedding layer to improve accuracy by identifying relationships between tokenised words within comments, allowing the model to understand sentence structure. I use a bidirectional LSTM layer to reinforce the models ability to understand sentences. The LSTM layer uses tanh activation function as is standard practice. The model then has 3 hidden layers with relu activation function. The model has a final output layer with sigmoid activation function to aid classification. The model is compiled with a Binary Crossentropy loss function since it is compatible with a sigmoid output layer and does well converting probability to binary output. I use an adam optimiser with the Binary crossentropy loss function.
## Make Predictions
If anyone wishes to use my code and make predictions they can find a gradio interface at the bottom of the toxicity.ipynb file to experiment with the model.
## Other files
labelled_data.csv is the original dataset i downloaded from Kaggle. Requirements.txt contains the packages required to run my code. Model.h5 is the saved version of my NN. Toxicity.ipynb is the main file containing all my code.
